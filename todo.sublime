TODO:

1. add exit case in node_split when gain=0 or approx 0. then check if matches python or CPU versions (it did not)
2. refactor into a class, put cublasHandle in class initialization
3. write prediction and error functions. make gpu versions
dont. use cublas for unc. sum(gpu_in_y)
4. refactor io/helper functions out of rforest.cpp
5. change class to do both cpu and gpu versions and time each

7. does cublas have functions to aid split/partition?
8. have the gpu only do on splits that are more than X points
9. convert tree to forest (w/random sampling from data (based on benchmarks?))
10. create depth limit to each tree (prevent over-training)
11. comment
12. have dataset name be taking by args
13. rename "col/row" to "point/feature" where appropriate
14. create free(tree) func
15. write predictions to csv file
16. read predictions from proper csv file
5. benchmark properly (cuda). support more than 1000 points and benchmark
gpu_mode as bool
