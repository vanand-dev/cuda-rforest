TODO:

1. add exit case in node_split when gain=0 or approx 0. then check if matches python or CPU versions
    it did not
2. refactor into a class, put cublasHandle in class initialization
3. write prediction and error functions
4. support more than 1000 points and benchmark
5. use cublas for unc. sum(gpu_in_y)
6. change class to do both cpu and gpu versions and time each
7. does cublas have functions to aid split/partition?
8. have the gpu only do on splits that are more than X points
9. convert tree to forest (w/random sampling from data (based on benchmarks?))
10. create depth limit to each tree (prevent over-training)
11. comment
12. have dataset name be taking by args
